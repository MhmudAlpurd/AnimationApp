{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hwrs002.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPYpTWwLjLJvwuFSz/Q4NJQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MhmudAlpurd/AnimationApp/blob/master/hwrs002.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBjCwuk7Ckhj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0db642b-9089-4ae0-a88b-36b702df6520"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZ7KbjcJC2UK",
        "outputId": "ffed7d40-8fd6-4061-ecb3-3e1dfebb7d05"
      },
      "source": [
        "cd /content/drive/My Drive/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssKnWPrsCVg4"
      },
      "source": [
        "import os, cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import numpy as np\n",
        "import random as rnd\n",
        "from tqdm import tqdm\n",
        "import pickle as pkl\n",
        "import PIL"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3TD6Yt2FW-K"
      },
      "source": [
        "**#path creation**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13F67kcgC915"
      },
      "source": [
        "DIRECTORY = 'Hampalai/HWRS_0_0_2/'\n",
        "CATEGORIES = ['plastics', 'metal' , 'paper' , 'glass']\n",
        "img_lbl_arr = []\n",
        "IMAGE_SIZE = 224"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXv_MCkUD33Z",
        "outputId": "cb7ef426-683e-49ed-cf0b-fa223cdd73aa"
      },
      "source": [
        "for category in tqdm(CATEGORIES):\n",
        "    category_folder = os.path.join(DIRECTORY, category)\n",
        "    img_label = CATEGORIES.index(category)\n",
        "    for img in os.listdir(category_folder):\n",
        "        image_path = os.path.join(category_folder, img)\n",
        "        #img_read = mpimg.imread(image_path)\n",
        "        #imgplot = plt.imshow(img_read)\n",
        "        #plt.show()\n",
        "        img_loaded_r = cv2.imread(image_path)\n",
        "        img_loaded = cv2.resize(img_loaded_r, (IMAGE_SIZE,IMAGE_SIZE))\n",
        "        img_lbl_arr.append([img_loaded, img_label])\n",
        "\n",
        "print(len(img_lbl_arr))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:11<00:00,  2.92s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1987\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZN1WDzjEvfg"
      },
      "source": [
        "**#shuffle the data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVRgYpodEoXC"
      },
      "source": [
        "rnd.shuffle(img_lbl_arr)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaKUdeBuE9xx"
      },
      "source": [
        "**#split image and labels and convert lst to arr**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xox8OWY9FAxh",
        "outputId": "b71968ef-7928-4974-8994-4e10e45230f8"
      },
      "source": [
        "X = []\n",
        "y = []\n",
        "\n",
        "for img_tensor, labels in img_lbl_arr:\n",
        "    X.append(img_tensor) #X = image tensor\n",
        "    y.append(labels) #y = label list\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "#pkl.dump(X, open('X.pkl', 'wb'))\n",
        "#pkl.dump(y, open('y.pkl', 'wb'))\n",
        "\n",
        "#X = pkl.load(open('X.pkl', 'rb'))\n",
        "#y = pkl.load(open('y.pkl', 'rb'))\n",
        "\n",
        "print(X.shape)\n",
        "#(1987, 150, 150, 3)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1987, 224, 224, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVjKreTOm-hF",
        "outputId": "1ec3da99-1247-4293-bda3-31f98fdd8033"
      },
      "source": [
        "from sklearn.model_selection import  train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "img_train, img_test, lbl_train, lbl_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
        "print('img_train:', len(img_train), type(img_train[0]), '| img_test:', len(img_test), '| lbl_train:', len(lbl_train), type(lbl_train[0]) , '| lbl_test:', len(lbl_test))\n",
        "\n",
        "# convert the labels from integers to vectors\n",
        "lbl_train = to_categorical(lbl_train, num_classes=4)\n",
        "lbl_test = to_categorical(lbl_test, num_classes=4)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "img_train: 1589 <class 'numpy.ndarray'> | img_test: 398 | lbl_train: 1589 <class 'numpy.int64'> | lbl_test: 398\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHvdFEKOGRVB"
      },
      "source": [
        "**#ImageDataGenerator**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJPmcTF-FE5Z"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from keras.models import Model\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
        "from keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from keras.optimizers import SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    featurewise_center=True, samplewise_center=True,\n",
        "    featurewise_std_normalization=True, samplewise_std_normalization=True,\n",
        "    rotation_range=20, width_shift_range=0.2,\n",
        "    height_shift_range=0.2, shear_range=0.1, zoom_range=0.2,\n",
        "    channel_shift_range=0.2, fill_mode='nearest', cval=0.1, horizontal_flip=True,\n",
        "    vertical_flip=True , rescale=True\n",
        "\n",
        ")\n",
        "\n",
        "datagen.fit(img_train)\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H92e8wZ2HXNb"
      },
      "source": [
        "**#model creation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxcd_s8EzwZM",
        "outputId": "5ea38883-6257-40ed-8593-7a61a6f7d581"
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras.models import Model\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
        "from keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from keras.optimizers import SGD\n",
        "\n",
        "#base_model = VGG16(include_top= True, weights='imagenet', input_shape=(IMAGE_SIZE,IMAGE_SIZE,3), pooling= 'avg', classes= 1000 , classifier_activation = None)\n",
        "#input_layer = Input(base_model)\n",
        "\n",
        "num_classes = 4\n",
        "\n",
        "myModel = Sequential([\n",
        "  layers.experimental.preprocessing.Rescaling(1./255, input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3)),\n",
        "  layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Flatten(),\n",
        "  layers.Dense(128, activation='relu'),\n",
        "  layers.Dense(num_classes)\n",
        "])\n",
        "\n",
        "\n",
        "from keras.optimizers import SGD, Adam\n",
        "from keras.losses import categorical_crossentropy\n",
        "optimizer_1 = SGD(lr=1e-4, momentum=0.99)\n",
        "optimizer_2 = Adam(lr=1e-3)\n",
        "\n",
        "#https://www.pyimagesearch.com/2017/12/11/image-classification-with-keras-and-deep-learning/\n",
        "\n",
        "#myModel.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=['accuracy'])\n",
        "myModel.compile(loss=\"binary_crossentropy\", optimizer='Adam', metrics=[\"accuracy\"])\n",
        "myModel.summary()\n",
        "\n",
        "myModel.fit(x=datagen.flow(img_train, lbl_train, batch_size=32),validation_data=(img_test, lbl_test), steps_per_epoch=len(img_train) // 32,epochs=96, verbose=1)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "rescaling_5 (Rescaling)      (None, 224, 224, 3)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 224, 224, 16)      448       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_15 (MaxPooling (None, 112, 112, 16)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 112, 112, 32)      4640      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_16 (MaxPooling (None, 56, 56, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 56, 56, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_17 (MaxPooling (None, 28, 28, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_5 (Flatten)          (None, 50176)             0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 128)               6422656   \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 4)                 516       \n",
            "=================================================================\n",
            "Total params: 6,446,756\n",
            "Trainable params: 6,446,756\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/96\n",
            "49/49 [==============================] - 89s 2s/step - loss: 0.6833 - accuracy: 0.2550 - val_loss: 11.4369 - val_accuracy: 0.2814\n",
            "Epoch 2/96\n",
            "49/49 [==============================] - 87s 2s/step - loss: 0.5632 - accuracy: 0.2897 - val_loss: 11.4369 - val_accuracy: 0.2764\n",
            "Epoch 3/96\n",
            "49/49 [==============================] - 86s 2s/step - loss: 0.5613 - accuracy: 0.2916 - val_loss: 11.4369 - val_accuracy: 0.2764\n",
            "Epoch 4/96\n",
            "49/49 [==============================] - 87s 2s/step - loss: 0.5627 - accuracy: 0.2942 - val_loss: 11.4369 - val_accuracy: 0.2764\n",
            "Epoch 5/96\n",
            "49/49 [==============================] - 87s 2s/step - loss: 0.5620 - accuracy: 0.3044 - val_loss: 11.4369 - val_accuracy: 0.2764\n",
            "Epoch 6/96\n",
            "49/49 [==============================] - 87s 2s/step - loss: 0.5621 - accuracy: 0.2897 - val_loss: 11.4369 - val_accuracy: 0.2764\n",
            "Epoch 7/96\n",
            "49/49 [==============================] - 87s 2s/step - loss: 0.5610 - accuracy: 0.2987 - val_loss: 11.4369 - val_accuracy: 0.2764\n",
            "Epoch 8/96\n",
            "49/49 [==============================] - 88s 2s/step - loss: 0.5614 - accuracy: 0.2974 - val_loss: 11.4369 - val_accuracy: 0.2764\n",
            "Epoch 9/96\n",
            "49/49 [==============================] - 87s 2s/step - loss: 0.5624 - accuracy: 0.2820 - val_loss: 11.4369 - val_accuracy: 0.2764\n",
            "Epoch 10/96\n",
            "49/49 [==============================] - 87s 2s/step - loss: 0.5599 - accuracy: 0.3076 - val_loss: 11.4369 - val_accuracy: 0.2764\n",
            "Epoch 11/96\n",
            "49/49 [==============================] - 87s 2s/step - loss: 0.5607 - accuracy: 0.3044 - val_loss: 11.4369 - val_accuracy: 0.2764\n",
            "Epoch 12/96\n",
            "49/49 [==============================] - 87s 2s/step - loss: 0.5592 - accuracy: 0.3064 - val_loss: 11.4369 - val_accuracy: 0.2764\n",
            "Epoch 13/96\n",
            "49/49 [==============================] - 88s 2s/step - loss: 0.5606 - accuracy: 0.2871 - val_loss: 11.4369 - val_accuracy: 0.2764\n",
            "Epoch 14/96\n",
            "49/49 [==============================] - 87s 2s/step - loss: 0.5598 - accuracy: 0.3070 - val_loss: 11.4369 - val_accuracy: 0.2764\n",
            "Epoch 15/96\n",
            "49/49 [==============================] - 89s 2s/step - loss: 0.5599 - accuracy: 0.3031 - val_loss: 11.4369 - val_accuracy: 0.2764\n",
            "Epoch 16/96\n",
            "49/49 [==============================] - 87s 2s/step - loss: 0.5596 - accuracy: 0.3064 - val_loss: 11.4369 - val_accuracy: 0.2764\n",
            "Epoch 17/96\n",
            "49/49 [==============================] - 87s 2s/step - loss: 0.5611 - accuracy: 0.3051 - val_loss: 11.4369 - val_accuracy: 0.2764\n",
            "Epoch 18/96\n",
            "49/49 [==============================] - 87s 2s/step - loss: 0.5605 - accuracy: 0.3038 - val_loss: 11.4369 - val_accuracy: 0.2764\n",
            "Epoch 19/96\n",
            "49/49 [==============================] - 87s 2s/step - loss: 0.5601 - accuracy: 0.3042 - val_loss: 11.4369 - val_accuracy: 0.2764\n",
            "Epoch 20/96\n",
            "49/49 [==============================] - 87s 2s/step - loss: 0.5600 - accuracy: 0.3038 - val_loss: 11.4369 - val_accuracy: 0.2764\n",
            "Epoch 21/96\n",
            "49/49 [==============================] - 88s 2s/step - loss: 0.5595 - accuracy: 0.3089 - val_loss: 11.4369 - val_accuracy: 0.2764\n",
            "Epoch 22/96\n",
            "49/49 [==============================] - 87s 2s/step - loss: 0.5602 - accuracy: 0.3019 - val_loss: 11.4369 - val_accuracy: 0.2764\n",
            "Epoch 23/96\n",
            "49/49 [==============================] - 87s 2s/step - loss: 0.5605 - accuracy: 0.3031 - val_loss: 11.4369 - val_accuracy: 0.2764\n",
            "Epoch 24/96\n",
            "49/49 [==============================] - 87s 2s/step - loss: 0.5604 - accuracy: 0.3025 - val_loss: 11.4369 - val_accuracy: 0.2764\n",
            "Epoch 25/96\n",
            "49/49 [==============================] - 87s 2s/step - loss: 0.5599 - accuracy: 0.3038 - val_loss: 11.4369 - val_accuracy: 0.2764\n",
            "Epoch 26/96\n",
            "49/49 [==============================] - 87s 2s/step - loss: 0.5608 - accuracy: 0.3038 - val_loss: 11.4369 - val_accuracy: 0.2764\n",
            "Epoch 27/96\n",
            "49/49 [==============================] - 87s 2s/step - loss: 0.5624 - accuracy: 0.3044 - val_loss: 11.4369 - val_accuracy: 0.2764\n",
            "Epoch 28/96\n",
            "49/49 [==============================] - 88s 2s/step - loss: 0.5603 - accuracy: 0.3025 - val_loss: 11.4369 - val_accuracy: 0.2764\n",
            "Epoch 29/96\n",
            "49/49 [==============================] - 87s 2s/step - loss: 0.5601 - accuracy: 0.3044 - val_loss: 11.4369 - val_accuracy: 0.2764\n",
            "Epoch 30/96\n",
            "49/49 [==============================] - 88s 2s/step - loss: 0.5600 - accuracy: 0.3044 - val_loss: 11.4369 - val_accuracy: 0.2764\n",
            "Epoch 31/96\n",
            "49/49 [==============================] - 87s 2s/step - loss: 0.5610 - accuracy: 0.3064 - val_loss: 11.4369 - val_accuracy: 0.2764\n",
            "Epoch 32/96\n",
            "49/49 [==============================] - 87s 2s/step - loss: 0.5601 - accuracy: 0.3038 - val_loss: 11.4369 - val_accuracy: 0.2764\n",
            "Epoch 33/96\n",
            "49/49 [==============================] - 88s 2s/step - loss: 0.5600 - accuracy: 0.3057 - val_loss: 11.4369 - val_accuracy: 0.2764\n",
            "Epoch 34/96\n",
            "49/49 [==============================] - 88s 2s/step - loss: 0.5600 - accuracy: 0.3051 - val_loss: 11.4369 - val_accuracy: 0.2764\n",
            "Epoch 35/96\n",
            "49/49 [==============================] - 88s 2s/step - loss: 0.5595 - accuracy: 0.3057 - val_loss: 11.4369 - val_accuracy: 0.2764\n",
            "Epoch 36/96\n",
            "49/49 [==============================] - 87s 2s/step - loss: 0.5596 - accuracy: 0.3044 - val_loss: 11.4369 - val_accuracy: 0.2764\n",
            "Epoch 37/96\n",
            "49/49 [==============================] - 87s 2s/step - loss: 0.5595 - accuracy: 0.3064 - val_loss: 11.4369 - val_accuracy: 0.2764\n",
            "Epoch 38/96\n",
            "49/49 [==============================] - 87s 2s/step - loss: 0.5604 - accuracy: 0.3031 - val_loss: 11.4369 - val_accuracy: 0.2764\n",
            "Epoch 39/96\n",
            "49/49 [==============================] - 87s 2s/step - loss: 0.5597 - accuracy: 0.3051 - val_loss: 11.4369 - val_accuracy: 0.2764\n",
            "Epoch 40/96\n",
            "49/49 [==============================] - 87s 2s/step - loss: 0.5603 - accuracy: 0.3070 - val_loss: 11.4369 - val_accuracy: 0.2764\n",
            "Epoch 41/96\n",
            "49/49 [==============================] - 87s 2s/step - loss: 0.5597 - accuracy: 0.3057 - val_loss: 11.4369 - val_accuracy: 0.2764\n",
            "Epoch 42/96\n",
            "49/49 [==============================] - 88s 2s/step - loss: 0.5599 - accuracy: 0.3012 - val_loss: 11.4369 - val_accuracy: 0.2764\n",
            "Epoch 43/96\n",
            "30/49 [=================>............] - ETA: 31s - loss: 0.5589 - accuracy: 0.3098"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHYxECODxt8s"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFRPPKZwHnzB"
      },
      "source": [
        "from keras.optimizers import SGD, Adam\n",
        "from keras.losses import categorical_crossentropy\n",
        "optimizer_1 = SGD(lr=1e-4, momentum=0.99)\n",
        "optimizer_2 = Adam(lr=1e-3)\n",
        "model.compile(loss='binary_crossentropy', optimizer=optimizer_1, metrics=['accuracy'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wWfExIOsqo-"
      },
      "source": [
        "EPOCHS = 100\n",
        "BS = 32\n",
        "# construct the training image generator for data augmentation\n",
        "aug = ImageDataGenerator(rotation_range=20, zoom_range=0.15,\n",
        "\twidth_shift_range=0.2, height_shift_range=0.2, shear_range=0.15,\n",
        "\thorizontal_flip=True, fill_mode=\"nearest\")\n",
        "# train the network\n",
        "H = model.fit_generator(aug.flow(trainX, trainY, batch_size=BS),\n",
        "\tvalidation_data=(testX, testY), steps_per_epoch=len(trainX) // BS,\n",
        "\tepochs=EPOCHS)\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzQwirc_l0S6"
      },
      "source": [
        "#https://www.pyimagesearch.com/2018/12/24/how-to-use-keras-fit-and-fit_generator-a-hands-on-tutorial/\n",
        "#The Keras deep learning libraries to train model:\n",
        "#1.fit\n",
        "#2.fit_generator\n",
        "#3.train_on_batch\n",
        "\n",
        "EPOCHS = 100\n",
        "BS = 128\n",
        "\n",
        "trained_model = myModel.fit(X, y ,batch_size= BS, epochs= EPOCHS,verbose=2, validation_split= 0.2, validation_data =(img_test, lbl_test) , validation_steps = 100,steps_per_epoch=100, shuffle=True)\n",
        "\n",
        "#history = myModel.fit(\n",
        "   # image_gen.flow(img_train, lbl_train, batch_size=BS, shuffle= True),\n",
        "   # validation_data =(img_test, lbl_test),\n",
        "    #validation_steps = 100,\n",
        "    #steps_per_epoch =  (len(img_train) // BS ),\n",
        "   # epochs = EPOCHS,\n",
        "    #verbose= 1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wVwqXU2HfMJ"
      },
      "source": [
        "**#Fitting**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCRe6yH7FTQC"
      },
      "source": [
        "input_layer = Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
        "conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same')(input_layer)\n",
        "pool1 = MaxPooling2D(pool_size = 2)(conv1)\n",
        "conv2 = Conv2D(32, 3, activation = 'relu', padding = 'same')(pool1)\n",
        "pool2 = MaxPooling2D(pool_size=2)(conv2)\n",
        "flat = Flatten()(pool2)\n",
        "out_layer = Dense(4, activation = 'softmax')(flat)\n",
        "myModel = Model(input_layer, out_layer)\n",
        "myModel.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zD2IPNZeHjlJ"
      },
      "source": [
        "**#Prediction**"
      ]
    }
  ]
}